\documentclass[10pt,letterpaper]{article}

\usepackage[pdftex]{graphicx}
\usepackage{amssymb} % allow blackboard bold (aka N,R,Q sets)
\usepackage{amsmath,amsthm}
\usepackage{mathtools}
\linespread{1.6}  % double spaces lines
\usepackage[left=1in,top=1in,right=1in,bottom=1in,nohead]{geometry}
\usepackage{cancel}
\usepackage{textcomp}
\usepackage{longtable}
\usepackage{rotating}
\allowdisplaybreaks
\usepackage{scrextend}
\usepackage{listings}
\usepackage{enumerate}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{url}
\usepackage{float}
\usepackage[usenames,dvipsnames]{color}

\newcommand{\partialderivative}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\alg}[1]{\textproc{#1}}
\def\R{\mathbb{R}}
\def\etal{\emph{et al.~}}
\def\range{\text{range}}
\def\alfa{\alpha}
\def\gama{\gamma}
\def\Span{\text{span}}

% for showing c++ code segments
\usepackage{listings,color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
    language=C++,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

\begin{document}

\linespread{1} % single spaces lines
\small \normalsize %% dumb, but have to do this for the prev to work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{CME 338 Final Project: \alg{USYMLQ} and \alg{USYMQR}}
\date{\today}
\author{Jui-Hsien Wang}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Body
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
In this project, we implemented the \alg{USYMLQ} and \alg{USYMQR} 
algorithms for solving large, sparse linear systems in templated C++. 
The algorithms are defined in the paper by Saunders \etal 
\cite{saunders:1988}, and in a closely-related paper by Reichel \etal 
\cite{reichel:2008}. These two methods were seen as extension to the 
Conjugate Gradient (\alg{CG}) and the Minimum Residual (\alg{MINRES}) 
methods for unsymmetric matrices. In addition, as pointed out in 
\cite{reichel:2008}, \alg{USYMQR} (or in their paper, 
``generalized \alg{LSQR}'', despite there is no special case in which 
\alg{USYMQR} reduces to \alg{LSQR}) can handle least-square problems 
(both of them can solve underdetermined system). \\

Our main contribution, in addition to providing with a tested 
implementation in yet another language, is to evaluate the algorithms 
on a variety of matrices, including \emph{The Unversity of Florida Sparse 
Matrix Collection} by Tim Davis \etal \cite{davis:2011}. 
Hundreds of different matrices originated from problems in many 
different areas (please see \cite{davis:2011} for the collection) were 
run through \alg{USYMLQ} and \alg{USYMQR} solvers. In addition, for 
a few selected problems, we compared the algorithms to the perhaps 
more well-known sparse linear system solvers including \alg{BiCGStab}, 
\alg{LSQR} and \alg{LSMR}. Since these implementations are only 
available to the author in Matlab, the comparison will be based on the 
number of iterations given similar error tolerance for the same problem. \\

\section{Tridiagonalization}
Given an unsymmetric, real $M \times N$ matrix $A$, the following 
decomposition exists (please see \cite{saunders:1988} for proof)
\begin{align}
    P^* A Q = T, \label{eq:PAQT}
\end{align}
where $P$ and $Q$ are orthogonal matrices and $T$ is a tridiagonal 
matrix. Note that if $A$ is symmetric then we can choose $P =Q$, then
$T$ is symmetric and the symmetric Lanczos process can be derived. This
decomposition is unique given a pair of starting vectors 
$p_1 \in \R^M$ and $q_1 \in \R^N$, which are the first columns of $P$
and $Q$, respectively. 
The algorithms can be easily extended to complex matrices, and focus will 
be given to only real matrices from now on. \\

At step $n$, let $P = P_n = (p_1, p_2, \cdots, p_n)$, 
$Q = Q_n = (q_1, q_2, \cdots, q_n)$, and 
\begin{align} 
    T = T_n = 
    \begin{bmatrix}
            \alfa_1 & \gama_2 & 0       & \cdots & 0      \\
            \beta_2 & \alfa_2 & \gama_3 & \cdots & 0      \\
            \vdots  & \ddots  & \ddots  & \ddots & \vdots \\
            0       & \cdots  & \beta_{n-1} & \alfa_{n-1} & \gama_n \\
            0       & \cdots  & 0           & \beta_n     & \alfa_n
    \end{bmatrix}. 
\end{align}
With this notation, we can derive an iterative tridiagonalization algorithm 
\textproc{TriDiag} following \cite{saunders:1988}. Instead of a three-term
recurrence relation in Lanczos process for symmetric matrices, we get two
three-term recurrence relations. When $A$ is symmetric, this algorithm 
falls back to the Lanczos process. Note that this is not a Krylov-subspace
method, but an analogy in the sense that the union of subspaces 
$\Span(Q_{2n})$ and $\Span(Q_{2n+1})$ contains the Krylov subspace generated
by $n$ steps of the symmetric Lanczos algorithm applied to the normal 
equations. In addition, they contain the space spanned by the intermediate 
vectors obtained if the matrix of the normal equations is not formed 
explicitly, but the multiplication by $A$ and $A^*$ are done in sequence 
\cite{saunders:1988}.

\begin{algorithm}[H]
\caption{Tridiagonalization of an Unsymmetric Matrix.}
\label{Alg:Tridiagonalization}
\begin{algorithmic}[1]
\Require $A$, and two arbitrary vectors $b \neq 0$, $c \neq 0$. 
\Function{TriDiag}{$A$, $b$, $c$}. 
\State $\beta_1 = \|b\|$, $\gama_1 = \|c\|$;
\State $p_0 = q_0 = 0$, $p_1 = b/\beta_1$, $q_1 = c/\gama_1$;
\For{$i = 1,2,3,\dots$}
    \State $u = A   q_i - \gama_i p_{i-1}$;
    \State $v = A^* p_i - \beta_i q_{i-1}$;
    \State $\alfa_i = p_i^* u$;
    \State $u -= \alfa_i   p_i$; 
    \State $v -= \alfa_i^* q_i$; 
    \State $\beta_{i+1} = \|u\|$; 
    \State $\gama_{i+1} = \|v\|$; 
    \If{$\beta_{i+1} = 0\,\,\textbf{or} \,\,\gama_{i+1} = 0$}
        \State break ; 
    \Else
        \State $p_{i+1} = u / \beta_{i+1}$;  
        \State $q_{i+1} = v / \gama_{i+1}$;  
    \EndIf
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solving Linear Systems}
The algorithm \textproc{TriDiag} can be used to solve the linear system
\begin{align} 
    A x = b, \label{eq:linear_system}
\end{align}
where $A$ is a general, unsymmetric $M\times N$ matrix. When $M > N$, 
we seek the solution to the least-square problem 
$\min\limits_x \|A x - b \|_2$. Note that in the tridiagonalization 
algorithm, $A$ is only defined by the operations $y \leftarrow Ax + y$ 
and $y \leftarrow A^* x + y$. Therefore both algorithms are suited for
solving large, sparse systems.\\

We start the algorithm by assigning $p_1 = r_0 / \|r_0$, where 
$r_0 = b - A x_0$ is the residual for an initial guess $x_0$.
$q_1$ can be chosen randomly. However, in \cite{reichel:2008}, it was
shown that it can be beneficial to set $q_1 \approx x$, which is 
generally unknown except in certain applications where estimation is
possible. After $j$ steps of \textproc{TriDiag}, we can approximate 
the solution to Eq.~\eqref{eq:linear_system} in the affine subspace 
$x_0 + \Span(Q_j)$. Depending on the desired properties of the residuals
at step $j$, $r_j = b - A x_j$, we have two types of methods: 
\alg{USYMLQ} and \alg{USYMQR} The following descriptions closely follow
the derivation in \cite{saunders:1988}.

\subsection{USYMLQ}
We denote $x_j^{cg}$ the solution given by \alg{USYMLQ} such that the 
residual vector $r_j^{cg} = b - A x_j^{cg}$ is orthogonal to 
$\Span(P_j)$. This is an oblique projection method and $x_j^{cg}$ can
be computed by: 
\begin{align} 
    & \text{Solve the $j\times j$ tridiagonal linear system 
            $T_j h_j^{cg} = \beta_1 e_1$}. \\
    & \text{Set $x_j^{cg} = x_0 + Q_j h_j^{cg}$}. 
\end{align}
An efficient implementation based on $LQ$ factorization of $T_j$ can
be found in \cite{saunders:1988}, using only two $M$-vectors and four
$N$-vectors. Updates at each step has complexity $\mathcal{O}(M N)$ due
to matrix-vector multiplications. 

\subsection{USYMQR}
We denote $x_j^{cr}$ the solution given by \alg{USYMQR} such that the 
residual vector $r_j^{cr} = b - A x_j^{cr}$ is minimal in the sense
that 
$\|r_j^{cr}\| = \min\limits_q\|b - A(x_0 + q)\|,\, q\in\Span(Q_j)$.
This implies the monoticity of $\|r_{j+1}^{cr}\| \leq \|r_{j}^{cr}\|$. 
In fact this method reduces to \alg{MINRES} when $A$ is symmetric, 
hence the name conjugate residual method. $x_j^{cr}$ can be computed
by: 
\begin{align} 
    & \text{Solve the $(j+1)\times j$ least-square problem
            $\min_{h_j^{cr}}\|S_j h_j^{cr} - \beta_1 e_1$}\|. \\
    & \text{Set $x_j^{cr} = x_0 + Q_j h_j^{cr}$},
\end{align}
where 
\begin{align}
    S_j = \begin{bmatrix}
        T_j \\
        \beta_{j+1}e_j^*
    \end{bmatrix}.
\end{align}
An efficient implementation based on $QR$ factorization of $S_j$ can
be found in \cite{saunders:1988}, One extra $N$-vector of storage is 
needed compared to USYMLQ.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation and Solver Design} 
The above algorithms were implemented in single-threaded C++. The 
implementation is abstracted and templated so different numerical 
linear algebra library can be used as long as it supports 
(1) matrix-vector/scalar-vector multiplication; (2) matrix transpose; 
(3) dot product, and (4) advanced initialization such as initializing 
zeros or random numbers (implemented as static methods). We used Eigen,
a free and open-sourced C++ template library for linear algebra 
\cite{sw:Eigen}. It is header-only and the source code of version 3.3.4
is included in the repository. Our \alg{USYMLQ} implementation was compared 
to a Matlab version and on average is $30\%$ slower, which is probably 
due to the underlying matrix-vector multiplication performance difference 
Matlab, which uses Intel MKL as backend, performs matrix-vector 
multiplication about $30\%$ faster than Eigen; this test was performed 
on a $2000\times 2000$ matrix (0.0024 sec vs 0.0036 sec). 
Although in the Eigen benchmark tests \cite{sw:EigenBenchmark}, it shows 
that for matrix-vector multiplication Eigen is faster than MKL. 
This does not seem to be the case given my clang compiler on Mac with 
O3 optimization. \\

\paragraph{Use of the solver is simple.} Following is a code snippet for 
how to use the solver given $A$ and $b$. It is easily configurable for 
solve mode (\texttt{USYMLQ} or \texttt{USYMQR}), logging output 
(e.g.~\texttt{std::cout}), logging verbose level, maximum timestep, and 
solver tolerance. In the snippet, \texttt{T\_Vector} and \texttt{T\_Matrix} 
are the template input arguments. 
\begin{lstlisting}
/* --------- SOLVER EXAMPLE --------- */         
// ... define A and b and x0 ... 
T_Vector x; 
T rnorm; 
USYM_Linear_Solver<T,T_Vector,T_Matrix> solver(A,b); 
solver.Initialize(x0); 
solver.Set_Mode(USYMQR);
solver.SetMaxIteration(2000);
solver.Set_Tol(1E-12, 1E-12);
solver.Solve(x, rnorm); 
\end{lstlisting}

\paragraph{Termination criteria} The solver returns an integer flag 
at termination to inform the user about the result of the solve. 
It is summarized in the following table.
\begin{table}[H]
\centerline{
\begin{tabular}{c|l}
 \textbf{Stop flag}  & \textbf{Reason for termination} \\
\hline
\texttt{ 0} & $x = 0$ is the exact solution.                   \\
            & No iterations were performed                     \\[1.3ex]
\texttt{ 1} & $\|Ax - b\|$ is sufficiently small,              \\  
            & given the values of $a_{tol}$ and $b_{tol}$.     \\[1.3ex]
\texttt{ 2} & $\|A^*(Ax - b)\|$ is sufficiently small,         \\
            & given the values of $a_{tol}$.                   \\[1.3ex]
\texttt{ 4} & $\|Ax - b\|$ is as small as seems reasonable.    \\[1.3ex]
\texttt{ 7} & The iteration limit \texttt{maxItn} was reached. \\[1.3ex]
\texttt{11} & The system $Ax = b$ is incompatible given 
              $b_{tol}$.
\end{tabular}}
\end{table}

\noindent The notion of ``sufficiently small'' is defined relatively to 
the problem.  In general, the error tolerance is defined by two 
user-specified numbers, $a_{tol}$ and $b_{tol}$. 
All the matrix norms defined below are Frobenius 
norms; all the vector norms are Euclidean norms. 
We make a further approximation that at step $j$, 
the norm of $A$ can be approximated by $\|A\| \approx \|T\|$.
The advantages of this approximation are that (1) its usually cheaper than 
running through the entire matrix ($\mathcal{O}(j)$ compared to 
$\mathcal{O}(MN)$ although the latter can be computed only once), and (2) 
$\|T_{j+1}\| \geq \|T_j\|$; if they are equal then we find the exact 
solution because \textproc{TriDiag} terminates, if they are not equal
then $\|T_j\|$ is increasing monotonically, which implies monotonically 
increasing solver tolerance (see below) and higher 
likelihood of termination. We explain the triggering conditions for each 
flag in the following paragraphs. \\

\noindent\texttt{Stop flag 1}: 
\begin{align}
    \frac{\|r\|}{\|b\|} < b_{tol} + a_{tol}\frac{\|A\| \|x\|}{\|b\|},
\end{align}

\noindent\texttt{Stop flag 2} (only in least-square problems):
\begin{align}
    \frac{\|A^*(Ax - b)\|}{\|A\|\|r\|} < a_{tol}
\end{align}

\noindent\texttt{Stop flag 11} (only in least-square problems): 
\begin{align}
    \frac{1}{\|r\|} < b_{tol}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results} 
We compare our \alg{USYMLQ} and \alg{USYMQR} implementations to the
widely used \alg{LSQR}, \alg{LSMR} and \alg{BiCGStab} algorithms. 
\alg{LSQR} and \alg{LSMR} are suitable for solving both square and 
rectangular systems (including under- and over-constrained systems), 
and \alg{BiCGStab} is suitable for square system. We used the Matlab 
built-in version of \alg{LSQR} and \alg{BiCGStab}, and community 
supported Matlab implementation of \alg{LSMR} \cite{sw:lsmr}. Because 
our implementation is in C++, it does not make much sense to compare 
the runtime speed across these algorithms. Instead, our comparison 
will be based on number of iterations mainly since all of these 
algorithms have matrix-vector multiplications at every step, which 
should be the main bottleneck for speed. \\

Some numerical experiments comparing a subset of these algorithms 
can be found in the literatures.
In \cite{saunders:1988}, a family of special block tridiagonal matrix 
was constructed to compared \alg{USYMLQ} and \alg{USYMQR} to 
\alg{LSQR}. This 1-parameter matrix family has the following shape,
\begin{align} 
    A = \begin{bmatrix}
         B & -I \\
        -I &  B & -I \\
           &  \ddots & \ddots & \ddots \\
           &         &   -I   &   B    &   -I \\
           &         &        &  -I    &    B 
        \end{bmatrix}, \quad
    B = \begin{bmatrix}
         4 & a \\
        b &  4 & a \\
           &  \ddots & \ddots & \ddots \\
           &         &   b   &   4    &   a \\
           &         &        &  b    &    4 
        \end{bmatrix},
\end{align}
where $a = -1+\delta$, and $b = -1 -\delta$. It was found that in general
\alg{LSQR} converges in less iterations when $\delta\geq 1.0$. In 
\cite{reichel:2008}, it was found that \alg{USYMQR} converged faster than
\alg{LSQR} when the solution cannot be well approximated by vectors in 
low-dimensional Krylov subspaces, for example, a near-constant function 
given by the discretization of Fredholm integral equation of the first kind. 
Furthermore, \cite{reichel:2008} observed that it can be beneficial to
choose the second starting vector in the tridiagonalization, $q_1$, 
so that it approximates the solution, $q_1 \approx x/\|x\|$. The use 
of this was demonstrated in an image deblurring problem, where 
$b \approx x$ and therefore was available. However, it is generally not true
one would have an approximate $x$ before the start of the algorithm.
Although these tests were well-designed to probe some properties of the 
algorithm, the selection of problems can cause bias in the evaluation. 
In the following sections, we tested \alg{USYMLQ} and \alg{USYMQR} on
(1) dense random matrices and (2) the UFL sparse matrix collection in
an attempt to gain more insights about the algorithms.

\subsection{Dense, random matrices}

The first set of test cases are square matrices that are dense and 
initialized randomly. The entries of the matrix are sampled uniformly
between $[-1, 1]$ through Eigen's advanced initialization methods. 
Right-hand-side vector $b$ is also initialized randomly and have 
values between $[-1, 1]$. Matrices of three different sizes were
initialized: $50\times 50$, $500 \times 500$, and $5000 \times 5000$.\\

Both \alg{USYMLQ}, \alg{USYMQR} determine the exact solution in at most
$N$ steps (see \cite{saunders:1988} for proof) in exact arithmetics, however, 
numerical errors cause the matrices $P$ and $Q$ to loss orthogonality
after some steps and in general equation \eqref{eq:PAQT} might not hold. 
Krylov subspace methods all have similar properties. It is therefore not
surprising to see that the number of iterations needed is higher than
the dimension of the problem, after which the residual drops rapidly 
to suggest a subspace containing the solution has been found. However, 
it is surprising to see that both \alg{USYMLQ} and \alg{USYMQR} found 
this space a bit earlier compared to \alg{LSQR} and \alg{LSMR}, 
suggesting that they might be more robust against orthogonality drift.
In general, we found that \alg{USYMQR} has much better convergence 
properties than \alg{USYMLQ}, which oscillates quite a bit and cannot
be stopped early. \alg{USYMQR}, due to its minimal residual guarantee,
$\|r_{j+1}^{cr}\| \leq \|r_{j}^{cr}\|$, has smoother convergence and
can be stopped early for higher error tolerance.
Please see figure \ref{fig:square_50}-\ref{fig:square_5000} for the
comparison.

\begin{figure}[H]
    \centering
    \includegraphics[width=5.0in]{figures/conv_all_benchmark_50.pdf}
    \caption{$M=50$, $N=50$, $A$ and $b$ are randomly initialized.}
    \label{fig:square_50}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=5.0in]{figures/conv_all_benchmark_500.pdf}
    \caption{$M=500$, $N=500$, $A$ and $b$ are randomly initialized.}
    \label{fig:square_500}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=5.0in]{figures/conv_all_benchmark_5000.pdf}
    \caption{$M=5000$, $N=5000$, $A$ and $b$ are randomly initialized.}
    \label{fig:square_5000}
\end{figure}

Next we look at three different random least-square problems ($M>N$) 
with $50\times 25$, $500\times 250$, and $5000\times 2500$ dimension. 
The same randomization as square systems was applied to generate $A$ and $b$. 
We found that for compatible systems ($Ax=b$), all algorithms converged
quickly with iteration count $j < N$ 
(see figures \ref{fig:compat_50_25}-\ref{fig:compat_5000_2500}). 
Except for the smallest test case ($M=50$, $N=25$), 
\alg{LSQR} and \alg{LSMR} converged faster than 
\alg{USYMQR} and \alg{USYMLQ}. Like square systems, \alg{USYMQR} 
demonstrates smoother convergence behavior than \alg{USYMLQ}. For incompatible
systems ($Ax\approx b$), we exclude \alg{USYMLQ} because it is not suitable 
for incompatible least-square problems. For incompatible system, we quantify
the convergence using the relative least-square error 
$r_{lsq} = \|A^*(Ax -b)\|/\|A\|$. The results are shown in figures 
\ref{fig:incompat_50_25}-\ref{fig:incompat_5000_2500}. We observed that there
seem to be an implementation-related issue to prevent the \alg{USYMQR} solver 
from converging below $10^{-8}$, and therefore our stopping criteria 
($a_{tol}=b_{tol}=10^{-12}$) was never triggered (see paragraph ``Termination
criteria'' for descriptions). Because \alg{USYMQR} only guarantees monotonically
decreasing $\|r\| = \|Ax_j -b\|$, we see that in all three cases the least-square
error actually increased. Interestingly, this seemingly implementation error
does not appear in either square or underdetermined system solve. If we 
compared the least-square solution returned by \alg{USYMQR} (termined with flag
\texttt{7}: iteration reached maximum) and \alg{LSQR}, 
we found that the solutions are indeed very close 
(figure \ref{fig:x_lsqr_usymqr}). These two facts suggest that the problem might
be inherent to the algorithm and not the implementation, although further
numerical and theoretical investigation is needed to draw a meaningful 
conclusion.

\begin{figure}[H]
    \centering
    \includegraphics[width=5.0in]{figures/conv_all_benchmark_compatible_50_25.pdf}
    \caption{$M=50$, $N=25$, $A$ and $b$ are randomly initialized.
             $A$ is compatible}
    \label{fig:compat_50_25}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=5.0in]{figures/conv_all_benchmark_compatible_500_250.pdf}
    \caption{$M=500$, $N=250$, $A$ and $b$ are randomly initialized. 
             $A$ is compatible}
    \label{fig:compat_500_250}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=5.0in]{figures/conv_all_benchmark_compatible_5000_2500.pdf}
    \caption{$M=5000$, $N=2500$, $A$ and $b$ are randomly initialized.
             $A$ is compatible}
    \label{fig:compat_5000_2500}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=5.0in]{figures/conv_all_benchmark_50_25.pdf}
    \caption{$M=50$, $N=25$, $A$ and $b$ are randomly initialized.
             $A$ is incompatible}
    \label{fig:incompat_50_25}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=5.0in]{figures/conv_all_benchmark_500_250.pdf}
    \caption{$M=500$, $N=250$, $A$ and $b$ are randomly initialized.
             $A$ is incompatible}
    \label{fig:incompat_500_250}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=5.0in]{figures/conv_all_benchmark_5000_2500.pdf}
    \caption{$M=5000$, $N=2500$, $A$ and $b$ are randomly initialized.
             $A$ is incompatible}
    \label{fig:incompat_5000_2500}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=5.0in]{figures/x_comparison_usymqr_lsqr.pdf}
    \caption{Comparison between the solutions returned by \alg{USYMQR} and 
             \alg{LSQR}. The model problem used was the $M=500$, $N=250$ 
             case.}
    \label{fig:x_lsqr_usymqr}
\end{figure}

\newpage
\subsection{UFL Sparse Matrix Collection}
The UFL sparse matrix collection \cite{davis:2011} is a large collection
of sparse matrices that is open to public. Most of the matrices come
from real-world problem, including structural engineering, computational 
fluid dynamics, computer graphics/vision, robotics, optimization, 
circuite simulations etc., to name just a few. This impressive 
collection provides an excellent source of test problems for our
\alg{USYMLQ} and \alg{USYMQR} algorithms. \\ 

The main goal for this testing is to see how many problems can the
\alg{USYMLQ} and \alg{USYMQR} handle. As of September, 2017, the UFL
collection has $1517$ matrices that has real entries. We restrict our 
discussion on only unsymmetric, real matrices but our algorithms and 
implementation can be readily extended to handle complex and integer
matrices (only test modules and proper linear algebra classes need to 
be rewritten).  Out of these matrices, we further restrict ourselves 
to problems that have less than $10000$ rows and $10000$ cols, mainly 
due to constraint on time and computational power. \\

We found in total $1017$ matrices that satisfy the constraints mentioned
above. These matrices were downloaded using an in-house Java
Script code in the Matrix Market format. I/O class for this format
in C++ was written in order to integrate them seamlessly into the 
pipeline. These matrices have various dimensions and we made no
attempt to probe its properties (e.g.~its rank, singular values, etc.,
are all unknown). Please see figure \ref{fig:ufl_size} for the distribution
of the problem sizes. To speed up the process (solving $2034$ problems
in total), we multi-threaded the testing program. The whole run can 
be completed in tens of minutes on a Xeon workstation. All problems were run
with the tolerance $a_{tol} = b_{tol} = 10^{-12}$, and maximum iteration 
$\texttt{maxItn} = 20\max(M,N)$. 

\begin{figure}[H]
    \centering
    \includegraphics[width=2.5in]{figures/ufl_problems_10k.pdf}
    \caption{This figure shows the distribution of problem sizes in the
             test set extracted from the UFL collection. Note that there
             are actually more under-determined problems than 
             over-determined problems. This bias is in the collection
             itself rather than our conscious selection. We ran both
             solvers, \alg{USYMLQ} and \alg{USYMQR}, on each of the 
             problem, generated in total $2034$ solver results that
             need processing.}
    \label{fig:ufl_size}
\end{figure}

We were able to obtain reasonable solutions for the majority of the problems. 
However, there are some problems that give our solver troubles. If 
residual reduction at $10^{-3}$ is needed, then \alg{USYMLQ} solves $79.8\%$
of the problems and \alg{USYMQR} solves $98.5\%$; if residual reduction
at $10^{-6}$ is needed, then \alg{USYMLQ} solves only $43.2\%$ of the problem
and \alg{USYMQR} solves $72.2\%$ of the problems. These statistics are deduced
from the data we show in table table \ref{tab:ufl_stopflags} and figure 
\ref{fig:ufl_reduction}. A rather large portion of them returned with 
stop flag \texttt{7}, which means iteration limits were reached. It is possible 
that we can improve the residual at the cost of more iterations, but this 
is not desired as the largest case we have in the test set is $9801\times 9801$,
and the iteration limit was set to $0.2$ million already. In general, our 
experiments seem to suggest that \alg{USYMQR} is more general compared to 
\alg{USYMLQ}, and should be tried first when a problem of unknown properties 
is encountered. In terms of computational speed, \alg{USYMQR} seems to be
slightly faster compared to \alg{USYMLQ} on many problems (see figure 
\ref{fig:ufl_speed}).

\begin{table}[H]
\centerline{
\begin{tabular}{c|c c}
    \textbf{Solver Type} & \textbf{Stop flag} & \textbf{Count} \\
    \hline
                         &         1          &      400       \\
          USYMLQ         &         7          &      594       \\
                         &         11         &      20        \\
    \hline
                         &         1          &      644       \\
          USYMQR         &         7          &      356       \\
                         &         11         &      14        
\end{tabular}}
\caption{Summary of stop flag.}
\label{tab:ufl_stopflags}
\end{table}


\begin{figure}[H]
    \minipage{0.5\textwidth}
    \includegraphics[width=\linewidth]{figures/ufl_residual_reduction_10k.pdf}
    \endminipage\hfill
    \minipage{0.5\textwidth}
    \includegraphics[width=\linewidth]{figures/ufl_residual_reduction_distribution_10k.pdf}
    \endminipage\hfill
    \caption{Left: Residual reduction plotted against problem size. 
             Both solvers are run for all problems 
             (blue: \alg{USYMQR}; red: \alg{USYMLQ}). Right: Cumulation count of 
             the residual reduction.}
    \label{fig:ufl_reduction}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=4.5in]{figures/ufl_timing_10k.pdf}
    \caption{This figure shows the statistics of the timing. We timed  
             the solver from initialization to final termination and 
             return the total elapsed time. This is done on a workstation
             with Intel Xeon E5-2690V3 2.6GHz 12-core processors.}
    \label{fig:ufl_speed}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion} 
In conclusion, we implemented the \alg{USYMLQ} and \alg{USYMQR} in C++. 
Both algorithms were compared to \alg{LSQR}, \alg{LSMR}, and 
\alg{BiCGStab} for solving dense random problems. We then constructed
a test set based on Tim Davis' UFL sparse matrix collection. About two
thousand instances were run to test the robustness of \alg{USYMLQ} and 
\alg{USYMQR}, and it was found that reasonable solutions can be obtained
for the majority of them. 

\bibliographystyle{unsrt}
\bibliography{documentation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

% \begin{algorithm}[H]
% \caption{Backtracking Algorithm.}
% \label{Alg:Backtracking_Algorithm}
% \begin{algorithmic}[1]
%     \REQUIRE $f(x)$, maximum step length $\bar{\alpha} > 0$, 
%     \STATE $\alpha \gets \bar{\alpha}$;
%     \REPEAT
%     \STATE $\alpha \gets \rho \alpha$; 
%     \UNTIL $f(x_k + \alpha p_k) \leq f(x_k) + c \alpha \nabla f_k^T p_k$; 
%     \RETURN $\alpha_k = \alpha$.
% \end{algorithmic}
% \end{algorithm}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=4.0in]{images/Ellipse_illustration.pdf}
%     \caption{Illustration of the ellipse covering problem}
%     \label{fig:Ellipse_illustration}
% \end{figure}

