\documentclass[10pt,letterpaper]{article}

\usepackage[pdftex]{graphicx}
\usepackage{amssymb} % allow blackboard bold (aka N,R,Q sets)
\usepackage{amsmath,amsthm}
\usepackage{mathtools}
\linespread{1.6}  % double spaces lines
\usepackage[left=1in,top=1in,right=1in,bottom=1in,nohead]{geometry}
\usepackage{cancel}
\usepackage{textcomp}
\usepackage{longtable}
\usepackage{rotating}
\allowdisplaybreaks
\usepackage{scrextend}
\usepackage{listings}
\usepackage{enumerate}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[usenames,dvipsnames]{color}

\newcommand{\partialderivative}[2]{\frac{\partial#1}{\partial#2}}
\def\R{\mathbb{R}}
\def\etal{\emph{et al.~}}
\def\range{\text{range}}
\def\alfa{\alpha}
\def\gama{\gamma}

\begin{document}

\linespread{1} % single spaces lines
\small \normalsize %% dumb, but have to do this for the prev to work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{CME 338 Final Project: USYMLQ and USYMQR}
\date{\today}
\author{Jui-Hsien Wang}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Body
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
In this project, we implemented the USYMLQ and USYMQR algorithms for 
solving large, sparse linear systems in templated C++. The algorithms
are defined in the paper by Saunders \etal \cite{saunders:1988}, and 
in a closely-related paper by Reichel \etal \cite{reichel:2008}. These 
two methods were seen as extension to the Conjugate Gradient (CG) and 
the Minimum Residual (MINRES) methods for unsymmetric matrices. 
In addition, as pointed out in
\cite{reichel:2008}, USYMQR (or in their paper, ``generalized LSQR'', 
despite there is no special case in which USYMQR reduces to LSQR) can 
handle least-square problems (both of them can solve underdetermined 
system). \\

Our main contribution, in addition to providing with a tested 
implementation in yet another language, is to evaluate the algorithms 
on a variety of matrices in \emph{The Unversity of Florida Sparse 
Matrix Collection} by Tim Davis \etal \cite{davis:2011}. 
Hundreds of different matrices originated from problems in many 
different areas (please see \cite{davis:2011} for the collection) were 
run through USYMLQ and USYMQR solvers. In addition, for a few selected 
problems, we compared the algorithms to the perhaps more well-known 
sparse linear system solvers including BiCGStab, LSQR and LSMR. Since 
these implementations are only available to the author in Matlab, the 
comparison will be based on the number of iterations given similar 
error tolerance for the same problem. \\

\section{Tridiagonalization}
Given an unsymmetric, real $M \times N$ matrix $A$, the following 
decomposition exists (please see \cite{saunders:1988} for proof)
\begin{align}
    P^* A Q = T,
\end{align}
where $P$ and $Q$ are orthogonal matrices and $T$ is a tridiagonal 
matrix. Note that if $A$ is symmetric then we can choose $P =Q$, then
$T$ is symmetric and the symmetric Lanczos process can be derived. This
decomposition is unique given a pair of starting vectors 
$p_1 \in \R^M$ and $q_1 \in \R^N$, which are the first columns of $P$
and $Q$, respectively. 
The algorithms can be easily extended to complex matrices, and focus will 
be given to only real matrices from now on. 

At step $n$, $P = P_n = (p_1, p_2, \cdots, p_n)$, 
$Q = Q_n = (q_1, q_2, \cdots, q_n)$, and 
\begin{align} 
    T = T_n = 
    \begin{bmatrix}
            \alfa_1 & \gama_2 & 0       & \cdots & 0      \\
            \beta_2 & \alfa_2 & \gama_3 & \cdots & 0      \\
            \vdots  & \ddots  & \ddots  & \ddots & \vdots \\
            0       & \cdots  & \beta_{n-1} & \alfa_{n-1} & \gama_n \\
            0       & \cdots  & 0           & \beta_n     & \alfa_n
    \end{bmatrix}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation} 

\subsection{USYMLQ}

\subsection{USYMQR}

% tridiagonalization
% LQ factorization
% initial condition
% termination condition
% algorithm


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{USYMQR Implementation} 

% tridiagonalization
% QR factorization
% initial condition
% termination condition
% algorithm


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results} 

%%% NEED THESE %%%
% random square matrices 
% matrices with specific eigenvalues 
% matrices with specific singular values 
% ill-conditioned matrices 
% sparse matrices
% inconsistent/consistent least square matrices 

%%% MAYBE %%%
% comparison with lsqr and lsmr? 
% comparison with the naive solve

%% on average, matlab uses 0.0024 sec on matrix-vector product for 
%% 2000-2000, and Eigen uses 0.0036


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion} 


\bibliographystyle{unsrt}
\bibliography{documentation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

% \begin{algorithm}[H]
% \caption{Backtracking Algorithm.}
% \label{Alg:Backtracking_Algorithm}
% \begin{algorithmic}[1]
%     \REQUIRE $f(x)$, maximum step length $\bar{\alpha} > 0$, 
%     \STATE $\alpha \gets \bar{\alpha}$;
%     \REPEAT
%     \STATE $\alpha \gets \rho \alpha$; 
%     \UNTIL $f(x_k + \alpha p_k) \leq f(x_k) + c \alpha \nabla f_k^T p_k$; 
%     \RETURN $\alpha_k = \alpha$.
% \end{algorithmic}
% \end{algorithm}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=4.0in]{images/Ellipse_illustration.pdf}
%     \caption{Illustration of the ellipse covering problem}
%     \label{fig:Ellipse_illustration}
% \end{figure}

